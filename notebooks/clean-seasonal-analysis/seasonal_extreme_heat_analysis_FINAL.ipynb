{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Extreme Heat Days Analysis - Final Implementation\n",
    "\n",
    "This notebook implements the **climatologically appropriate** extreme heat day calculation using seasonal percentiles.\n",
    "\n",
    "## üî¨ **Methodology:**\n",
    "```\n",
    "Surface_Heat_Day = 1 if LST_daily_max > max(LST_abs, LST_rel)\n",
    "```\n",
    "Where:\n",
    "- **LST_abs**: Absolute threshold (e.g., 35¬∞C)\n",
    "- **LST_rel**: percentile_X{LST_daily_max for calendar_day over reference period}\n",
    "\n",
    "## üéØ **Scientific Advantages:**\n",
    "- **Seasonal context**: July heat vs January heat appropriately weighted\n",
    "- **Climatological accuracy**: Follows meteorological best practices\n",
    "- **Sensitive detection**: Can identify seasonal heat anomalies\n",
    "- **Day-specific baselines**: Each day compared to its climatological context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import ee\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Initialize Earth Engine\n",
    "try:\n",
    "    ee.Initialize(project='tl-cities')\n",
    "    print('‚úÖ Earth Engine initialized successfully')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Earth Engine initialization failed: {e}')\n",
    "    raise\n",
    "\n",
    "print('üì¶ Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis configuration\n",
    "CONFIG = {\n",
    "    'analysis_year': 2020,\n",
    "    'reference_start': 2003,\n",
    "    'reference_end': 2019,\n",
    "    'absolute_threshold': 35.0,  # ¬∞C\n",
    "    'percentile_threshold': 90.0,  # percentile\n",
    "    'max_temp_filter': 50.0,  # ¬∞C - filter extreme outliers\n",
    "    'roi_bounds': [-38.7, -13.1, -38.3, -12.8],  # Salvador, Brazil [west, south, east, north]\n",
    "    'scale': 1000  # meters\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../outputs/final_seasonal_analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print('‚öôÔ∏è Configuration:')\n",
    "for key, value in CONFIG.items():\n",
    "    print(f'   {key}: {value}')\n",
    "print(f'üìÅ Output directory: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define region of interest\n",
    "west, south, east, north = CONFIG['roi_bounds']\n",
    "roi = ee.Geometry.Rectangle([west, south, east, north])\n",
    "\n",
    "# Calculate area\n",
    "area_km2 = roi.area().divide(1000000).getInfo()\n",
    "print(f'üéØ ROI: Salvador, Brazil region')\n",
    "print(f'   Area: {area_km2:.1f} km¬≤')\n",
    "print(f'   Bounds: W={west}, E={east}, S={south}, N={north}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gshtd_collection(geometry, start_date, end_date):\n",
    "    \"\"\"Get GSHTD temperature collection for Latin America\"\"\"\n",
    "    collection_id = \"projects/sat-io/open-datasets/global-daily-air-temp/latin_america\"\n",
    "    \n",
    "    collection = ee.ImageCollection(collection_id)\n",
    "    filtered = (collection.filterDate(start_date, end_date)\n",
    "                         .filterBounds(geometry)\n",
    "                         .filter(ee.Filter.eq('prop_type', 'tmax')))\n",
    "    \n",
    "    # Apply temperature scaling and quality filtering\n",
    "    def process_image(img):\n",
    "        # Scale from Kelvin*10 to Celsius and apply quality filter\n",
    "        temp_celsius = img.select('b1').divide(10.0)\n",
    "        \n",
    "        # Filter extreme values (likely data quality issues)\n",
    "        temp_filtered = temp_celsius.updateMask(\n",
    "            temp_celsius.gte(-20).And(temp_celsius.lte(CONFIG['max_temp_filter']))\n",
    "        )\n",
    "        \n",
    "        return temp_filtered.rename('temperature').clip(geometry).copyProperties(img, ['system:time_start'])\n",
    "    \n",
    "    return filtered.map(process_image)\n",
    "\n",
    "print('üõ†Ô∏è Data extraction functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extracting temperature data with monthly chunking...\n",
      "   Years: 2003-2020 (18 years)\n",
      "   Expected pixels per image: 510\n"
     ]
    }
   ],
   "source": [
    "# Extract temperature data with monthly chunking for large datasets\n",
    "print('üîÑ Extracting temperature data with monthly chunking...')\n",
    "\n",
    "# Years to extract: reference period + analysis year\n",
    "all_years = list(range(CONFIG['reference_start'], CONFIG['reference_end'] + 1)) + [CONFIG['analysis_year']]\n",
    "all_years = sorted(list(set(all_years)))  # Remove duplicates\n",
    "\n",
    "print(f'   Years: {all_years[0]}-{all_years[-1]} ({len(all_years)} years)')\n",
    "\n",
    "# Test pixel count\n",
    "test_collection = get_gshtd_collection(roi, f'{CONFIG[\"analysis_year\"]}-01-01', f'{CONFIG[\"analysis_year\"]}-01-02')\n",
    "first_image = test_collection.first()\n",
    "pixel_count = first_image.select('temperature').reduceRegion(\n",
    "    reducer=ee.Reducer.count(),\n",
    "    geometry=roi,\n",
    "    scale=CONFIG['scale'],\n",
    "    maxPixels=1e9\n",
    ").getInfo()\n",
    "\n",
    "expected_pixels = pixel_count.get('temperature', 0)\n",
    "print(f'   Expected pixels per image: {expected_pixels}')\n",
    "\n",
    "if expected_pixels == 0:\n",
    "    raise ValueError('No pixels found in ROI - check region coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Extracting data month by month...\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2003 ===\n",
      "   Month 01... 14756 obs    Month 02... 13328 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2003: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2004 ===\n",
      "   Month 01... 14756 obs    Month 02... 13804 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14280 obs \n",
      "   ‚úÖ Year 2004: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2005 ===\n",
      "   Month 01... 14756 obs    Month 02... 13328 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2005: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2006 ===\n",
      "   Month 01... 14280 obs    Month 02... 13328 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2006: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2007 ===\n",
      "   Month 01... 14756 obs    Month 02... 13328 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2007: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2008 ===\n",
      "   Month 01... 14725 obs    Month 02... 13775 obs    Month 03... 14725 obs    Month 04... 14250 obs    Month 05... 14725 obs    Month 06... 14250 obs    Month 07... 14725 obs    Month 08... 14725 obs    Month 09... 14250 obs    Month 10... 14725 obs    Month 11... 14250 obs    Month 12... 14250 obs \n",
      "   ‚úÖ Year 2008: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2009 ===\n",
      "   Month 01... 14756 obs    Month 02... 13328 obs    Month 03... 15732 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2009: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2010 ===\n",
      "   Month 01... 15779 obs    Month 02... 14252 obs    Month 03... 15779 obs    Month 04... 15270 obs    Month 05... 15779 obs    Month 06... 15270 obs    Month 07... 15779 obs    Month 08... 15779 obs    Month 09... 15270 obs    Month 10... 15779 obs    Month 11... 15270 obs    Month 12... 15779 obs \n",
      "   ‚úÖ Year 2010: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2011 ===\n",
      "   Month 01... 14756 obs    Month 02... 13328 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2011: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2012 ===\n",
      "   Month 01... 14756 obs    Month 02... 13804 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14756 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 14280 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14280 obs \n",
      "   ‚úÖ Year 2012: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2013 ===\n",
      "   Month 01... 14756 obs    Month 02... 13328 obs    Month 03... 14756 obs    Month 04... 14280 obs    Month 05... 14283 obs    Month 06... 14280 obs    Month 07... 14756 obs    Month 08... 14756 obs    Month 09... 13328 obs    Month 10... 14756 obs    Month 11... 14280 obs    Month 12... 14756 obs \n",
      "   ‚úÖ Year 2013: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2014 ===\n",
      "   Month 01... 15686 obs    Month 02... 14168 obs    Month 03... 15686 obs    Month 04... 15180 obs    Month 05... 15686 obs    Month 06... 15180 obs    Month 07... 15686 obs    Month 08... 15686 obs    Month 09... 15180 obs    Month 10... 15686 obs    Month 11... 15180 obs    Month 12... 15686 obs \n",
      "   ‚úÖ Year 2014: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2015 ===\n",
      "   Month 01... 15717 obs    Month 02... 14196 obs    Month 03... 15717 obs    Month 04... 15210 obs    Month 05... 15717 obs    Month 06... 15210 obs    Month 07... 15717 obs    Month 08... 15717 obs    Month 09... 15210 obs    Month 10... 15210 obs    Month 11... 15210 obs    Month 12... 15717 obs \n",
      "   ‚úÖ Year 2015: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2016 ===\n",
      "   Month 01... 15686 obs    Month 02... 14674 obs    Month 03... 15686 obs    Month 04... 15180 obs    Month 05... 15686 obs    Month 06... 15180 obs    Month 07... 15686 obs    Month 08... 15686 obs    Month 09... 15180 obs    Month 10... 15686 obs    Month 11... 15180 obs    Month 12... 15180 obs \n",
      "   ‚úÖ Year 2016: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2017 ===\n",
      "   Month 01... 15717 obs    Month 02... 14196 obs    Month 03... 15717 obs    Month 04... 15210 obs    Month 05... 15210 obs    Month 06... 13799 obs    Month 07... 15717 obs    Month 08... 15717 obs    Month 09... 12675 obs    Month 10... 15717 obs    Month 11... 15210 obs    Month 12... 15717 obs \n",
      "   ‚úÖ Year 2017: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2018 ===\n",
      "   Month 01... 15717 obs    Month 02... 14196 obs    Month 03... 15717 obs    Month 04... 15210 obs    Month 05... 15717 obs    Month 06... 15210 obs    Month 07... 14703 obs    Month 08... 15717 obs    Month 09... 15210 obs    Month 10... 15717 obs    Month 11... 15210 obs    Month 12... 15717 obs \n",
      "   ‚úÖ Year 2018: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2019 ===\n",
      "   Month 01... 15717 obs    Month 02... 14196 obs    Month 03... 15717 obs    Month 04... 15210 obs    Month 05... 15717 obs    Month 06... 15192 obs    Month 07... 14703 obs    Month 08... 14673 obs    Month 09... 14196 obs    Month 10... 12879 obs    Month 11... 14703 obs    Month 12... 15717 obs \n",
      "   ‚úÖ Year 2019: 12 successful months\n",
      "\n",
      "üìÖ === EXTRACTING YEAR 2020 ===\n",
      "   Month 01... 15717 obs    Month 02... 14703 obs    Month 03... 15717 obs    Month 04... 15210 obs    Month 05... 15717 obs    Month 06... 15210 obs    Month 07... 15717 obs    Month 08... 15717 obs    Month 09... 15210 obs    Month 10... 15717 obs    Month 11... 15210 obs    Month 12... 15210 obs \n",
      "   ‚úÖ Year 2020: 12 successful months\n",
      "\n",
      "üìä EXTRACTION SUMMARY:\n",
      "   Successful months: 216\n",
      "   Failed months: 0\n",
      "   Total observations: 3,204,141\n"
     ]
    }
   ],
   "source": [
    "# Extract data using monthly chunking to handle large datasets\n",
    "print('üìä Extracting data month by month...')\n",
    "\n",
    "all_dataframes = []\n",
    "extraction_stats = {'successful': 0, 'failed': 0, 'total_obs': 0}\n",
    "\n",
    "for extract_year in all_years:\n",
    "    print(f'\\nüìÖ === EXTRACTING YEAR {extract_year} ===')\n",
    "    year_dataframes = []\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        print(f'   Month {month:02d}...', end=' ')\n",
    "        \n",
    "        # Monthly date range\n",
    "        start_date = f'{extract_year}-{month:02d}-01'\n",
    "        if month == 12:\n",
    "            end_date = f'{extract_year+1}-01-01'\n",
    "        else:\n",
    "            end_date = f'{extract_year}-{month+1:02d}-01'\n",
    "        \n",
    "        try:\n",
    "            # Get monthly collection\n",
    "            month_collection = get_gshtd_collection(roi, start_date, end_date)\n",
    "            month_count = month_collection.size().getInfo()\n",
    "            \n",
    "            if month_count == 0:\n",
    "                print('No images', end=' ')\n",
    "                extraction_stats['failed'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Extract with appropriate scale\n",
    "            month_estimated = expected_pixels * month_count\n",
    "            month_scale = 2000 if month_estimated > 800000 else CONFIG['scale']\n",
    "            \n",
    "            region_data = month_collection.getRegion(\n",
    "                geometry=roi,\n",
    "                scale=month_scale,\n",
    "                crs='EPSG:4326'\n",
    "            ).getInfo()\n",
    "            \n",
    "            if len(region_data) > 1:\n",
    "                header = region_data[0]\n",
    "                data_rows = region_data[1:]\n",
    "                \n",
    "                df_month = pd.DataFrame(data_rows, columns=header)\n",
    "                df_month['time'] = pd.to_datetime(df_month['time'], unit='ms')\n",
    "                df_month = df_month.dropna(subset=['temperature'])\n",
    "                df_month = df_month[df_month['temperature'] <= CONFIG['max_temp_filter']]\n",
    "                \n",
    "                if len(df_month) > 0:\n",
    "                    year_dataframes.append(df_month)\n",
    "                    extraction_stats['successful'] += 1\n",
    "                    extraction_stats['total_obs'] += len(df_month)\n",
    "                    print(f'{len(df_month)} obs', end=' ')\n",
    "                else:\n",
    "                    print('No valid data', end=' ')\n",
    "                    extraction_stats['failed'] += 1\n",
    "            else:\n",
    "                print('No data returned', end=' ')\n",
    "                extraction_stats['failed'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Error: {str(e)[:30]}...', end=' ')\n",
    "            extraction_stats['failed'] += 1\n",
    "    \n",
    "    # Add year's data to main collection\n",
    "    if year_dataframes:\n",
    "        print(f'\\n   ‚úÖ Year {extract_year}: {len(year_dataframes)} successful months')\n",
    "        all_dataframes.extend(year_dataframes)\n",
    "    else:\n",
    "        print(f'\\n   ‚ùå Year {extract_year}: No data extracted')\n",
    "\n",
    "print(f'\\nüìä EXTRACTION SUMMARY:')\n",
    "print(f'   Successful months: {extraction_stats[\"successful\"]}')\n",
    "print(f'   Failed months: {extraction_stats[\"failed\"]}')\n",
    "print(f'   Total observations: {extraction_stats[\"total_obs\"]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your successfully extracted DataFrame and convert properly to xarray\n",
    "print('üîÑ Using your successfully extracted DataFrame...')\n",
    "\n",
    "# Check if the DataFrame exists from previous session\n",
    "if 'df' in globals() and len(df) > 100000:\n",
    "   print(f'‚úÖ Using existing DataFrame: {len(df):,} observations')\n",
    "\n",
    "   # Clean the DataFrame\n",
    "   df_clean = df.copy()\n",
    "   df_clean['time'] = pd.to_datetime(df_clean['time'])\n",
    "   df_clean['temperature'] = pd.to_numeric(df_clean['temperature'], errors='coerce')\n",
    "   df_clean = df_clean.dropna(subset=['temperature'])\n",
    "\n",
    "   print(f'   After cleaning: {len(df_clean):,} valid observations')\n",
    "\n",
    "   # Convert to xarray using from_dataframe() - handles sparse data properly\n",
    "   temperature_data = xr.Dataset.from_dataframe(\n",
    "       df_clean.set_index(['time', 'latitude', 'longitude'])[['temperature']]\n",
    "   )\n",
    "\n",
    "   print(f'   üìä Xarray dataset created: {dict(temperature_data.dims)}')\n",
    "   print(f'   Valid temperature values: {(~temperature_data.temperature.isnull()).sum().values:,}')\n",
    "\n",
    "else:\n",
    "   print('‚ùå No DataFrame found. Please run the data extraction first.')\n",
    "   raise ValueError('No df DataFrame available')\n",
    "\n",
    "print('‚úÖ Data ready for analysis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>time</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LatinAmerica_Ta_SVCMsp_2003TMAX_001</td>\n",
       "      <td>-38.694931</td>\n",
       "      <td>-13.04803</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>31.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LatinAmerica_Ta_SVCMsp_2003TMAX_002</td>\n",
       "      <td>-38.694931</td>\n",
       "      <td>-13.04803</td>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>28.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LatinAmerica_Ta_SVCMsp_2003TMAX_003</td>\n",
       "      <td>-38.694931</td>\n",
       "      <td>-13.04803</td>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>31.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LatinAmerica_Ta_SVCMsp_2003TMAX_004</td>\n",
       "      <td>-38.694931</td>\n",
       "      <td>-13.04803</td>\n",
       "      <td>2003-01-04</td>\n",
       "      <td>32.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LatinAmerica_Ta_SVCMsp_2003TMAX_005</td>\n",
       "      <td>-38.694931</td>\n",
       "      <td>-13.04803</td>\n",
       "      <td>2003-01-05</td>\n",
       "      <td>32.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  longitude  latitude       time  \\\n",
       "0  LatinAmerica_Ta_SVCMsp_2003TMAX_001 -38.694931 -13.04803 2003-01-01   \n",
       "1  LatinAmerica_Ta_SVCMsp_2003TMAX_002 -38.694931 -13.04803 2003-01-02   \n",
       "2  LatinAmerica_Ta_SVCMsp_2003TMAX_003 -38.694931 -13.04803 2003-01-03   \n",
       "3  LatinAmerica_Ta_SVCMsp_2003TMAX_004 -38.694931 -13.04803 2003-01-04   \n",
       "4  LatinAmerica_Ta_SVCMsp_2003TMAX_005 -38.694931 -13.04803 2003-01-05   \n",
       "\n",
       "   temperature  \n",
       "0         31.9  \n",
       "1         28.5  \n",
       "2         31.4  \n",
       "3         32.3  \n",
       "4         32.7  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seasonal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean xarray-based seasonal analysis\n",
    "print('üî¨ Performing seasonal extreme heat analysis...')\n",
    "\n",
    "# Get unique years\n",
    "years_available = np.unique(temperature_data.time.dt.year.values)\n",
    "print(f'Years available: {sorted(years_available)}')\n",
    "\n",
    "# Add day of year coordinate\n",
    "temperature_data = temperature_data.assign_coords(dayofyear=temperature_data.time.dt.dayofyear)\n",
    "\n",
    "# Split data cleanly\n",
    "analysis_data = temperature_data.sel(time=str(CONFIG['analysis_year']))\n",
    "reference_data = temperature_data.sel(time=slice(f'{CONFIG[\"reference_start\"]}-01-01', f'{CONFIG[\"reference_end\"]}-12-31'))\n",
    "\n",
    "print(f'Analysis period ({CONFIG[\"analysis_year\"]}): {len(analysis_data.time)} days')\n",
    "print(f'Reference period ({CONFIG[\"reference_start\"]}-{CONFIG[\"reference_end\"]}): {len(reference_data.time)} days')\n",
    "\n",
    "if len(reference_data.time) == 0:\n",
    "    raise ValueError('‚ùå No reference data found - check data extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seasonal percentiles using proper xarray operations\n",
    "print('üìä Calculating seasonal percentiles...')\n",
    "\n",
    "percentile_value = CONFIG['percentile_threshold'] / 100\n",
    "\n",
    "# Group by day of year and calculate percentiles - the proper xarray way\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')  # Suppress numpy warnings\n",
    "    seasonal_percentiles = reference_data.groupby('dayofyear').quantile(\n",
    "        percentile_value, dim='time', skipna=True\n",
    "    ).temperature\n",
    "\n",
    "print(f'‚úÖ Seasonal percentiles calculated: {seasonal_percentiles.dims}')\n",
    "print(f'   Days with valid percentiles: {(~seasonal_percentiles.isnull()).sum().values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply seasonal methodology\n",
    "print('üî• Calculating seasonal heat days...')\n",
    "\n",
    "abs_threshold = CONFIG['absolute_threshold']\n",
    "\n",
    "# Calculate daily thresholds using broadcasting\n",
    "daily_thresholds = xr.where(\n",
    "    seasonal_percentiles > abs_threshold,\n",
    "    seasonal_percentiles,\n",
    "    abs_threshold\n",
    ")\n",
    "\n",
    "# Map thresholds to analysis days\n",
    "analysis_thresholds = daily_thresholds.sel(dayofyear=analysis_data.dayofyear)\n",
    "\n",
    "# Calculate heat days\n",
    "seasonal_heat_days = (analysis_data.temperature > analysis_thresholds).sum(dim='time')\n",
    "\n",
    "print(f'‚úÖ Seasonal heat days calculated')\n",
    "\n",
    "# Calculate annual method for comparison\n",
    "print('üìä Calculating annual method for comparison...')\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    annual_percentile = reference_data.temperature.quantile(percentile_value, dim='time', skipna=True)\n",
    "    \n",
    "annual_threshold = xr.where(annual_percentile > abs_threshold, annual_percentile, abs_threshold)\n",
    "annual_heat_days = (analysis_data.temperature > annual_threshold).sum(dim='time')\n",
    "\n",
    "print(f'‚úÖ Annual comparison calculated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "seasonal_mean = float(seasonal_heat_days.mean().values)\n",
    "annual_mean = float(annual_heat_days.mean().values)\n",
    "seasonal_max = float(seasonal_heat_days.max().values)\n",
    "annual_max = float(annual_heat_days.max().values)\n",
    "seasonal_pixels = int((seasonal_heat_days > 0).sum().values)\n",
    "annual_pixels = int((annual_heat_days > 0).sum().values)\n",
    "total_pixels = int(seasonal_heat_days.count().values)\n",
    "\n",
    "print(f'\\nüìä METHODOLOGY COMPARISON:')\n",
    "print(f'üî¨ Seasonal Method:')\n",
    "print(f'   Mean heat days: {seasonal_mean:.1f}')\n",
    "print(f'   Max heat days: {seasonal_max:.0f}')\n",
    "print(f'   Pixels with >0 heat days: {seasonal_pixels} of {total_pixels} ({seasonal_pixels/total_pixels*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüìÖ Annual Method:')\n",
    "print(f'   Mean heat days: {annual_mean:.1f}')\n",
    "print(f'   Max heat days: {annual_max:.0f}')\n",
    "print(f'   Pixels with >0 heat days: {annual_pixels} of {total_pixels} ({annual_pixels/total_pixels*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüìà Difference (Seasonal - Annual):')\n",
    "print(f'   Mean: {seasonal_mean - annual_mean:+.1f} days')\n",
    "print(f'   Pixels: {seasonal_pixels - annual_pixels:+.0f}')\n",
    "\n",
    "print(f'\\n‚úÖ Seasonal analysis complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "print('üìä Creating visualizations...')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Seasonal Heat Days\n",
    "seasonal_heat_days.plot(ax=axes[0,0], cmap='Reds', add_colorbar=True,\n",
    "                       cbar_kwargs={'label': 'Heat Days'})\n",
    "axes[0,0].set_title('üî¨ Seasonal Method Heat Days', fontweight='bold', fontsize=14)\n",
    "axes[0,0].set_xlabel('Longitude')\n",
    "axes[0,0].set_ylabel('Latitude')\n",
    "\n",
    "# Annual Heat Days  \n",
    "annual_heat_days.plot(ax=axes[0,1], cmap='Reds', add_colorbar=True,\n",
    "                     cbar_kwargs={'label': 'Heat Days'})\n",
    "axes[0,1].set_title('üìÖ Annual Method Heat Days', fontweight='bold', fontsize=14)\n",
    "axes[0,1].set_xlabel('Longitude')\n",
    "axes[0,1].set_ylabel('Latitude')\n",
    "\n",
    "# Difference Map\n",
    "difference = seasonal_heat_days - annual_heat_days\n",
    "difference.plot(ax=axes[1,0], cmap='RdBu_r', add_colorbar=True,\n",
    "               cbar_kwargs={'label': 'Difference (Seasonal - Annual)'})\n",
    "axes[1,0].set_title('üìà Method Difference', fontweight='bold', fontsize=14)\n",
    "axes[1,0].set_xlabel('Longitude')\n",
    "axes[1,0].set_ylabel('Latitude')\n",
    "\n",
    "# Distribution comparison\n",
    "seasonal_flat = seasonal_heat_days.values.flatten()\n",
    "annual_flat = annual_heat_days.values.flatten() \n",
    "seasonal_clean = seasonal_flat[~np.isnan(seasonal_flat)]\n",
    "annual_clean = annual_flat[~np.isnan(annual_flat)]\n",
    "\n",
    "axes[1,1].hist(seasonal_clean, bins=20, alpha=0.7, color='red',\n",
    "              edgecolor='black', label='Seasonal Method')\n",
    "axes[1,1].hist(annual_clean, bins=20, alpha=0.5, color='blue',\n",
    "              edgecolor='black', label='Annual Method')\n",
    "axes[1,1].set_title('Heat Days Distribution Comparison', fontweight='bold', fontsize=14)\n",
    "axes[1,1].set_xlabel('Heat Days per Pixel')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display correlation\n",
    "correlation = np.corrcoef(annual_clean, seasonal_clean)[0,1]\n",
    "print(f'\\nüìà Correlation between methods: {correlation:.3f}')\n",
    "\n",
    "if correlation > 0.8:\n",
    "    print('   ‚úÖ Strong positive correlation - methods generally agree')\n",
    "elif correlation > 0.5:\n",
    "    print('   ‚ö†Ô∏è Moderate correlation - some differences in spatial patterns')\n",
    "else:\n",
    "    print('   üîç Low correlation - significant methodological differences')\n",
    "\n",
    "print('‚úÖ Visualization complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to multiple formats\n",
    "print('üìÅ Exporting results...')\n",
    "\n",
    "# 1. Create NetCDF with proper CRS\n",
    "print('   üì¶ Creating NetCDF file...')\n",
    "\n",
    "# Prepare spatial results\n",
    "results = {\n",
    "    'seasonal_heat_days': seasonal_heat_days.fillna(0),\n",
    "    'annual_heat_days': annual_heat_days.fillna(0),\n",
    "    'heat_days_difference': (seasonal_heat_days - annual_heat_days).fillna(0),\n",
    "    'seasonal_percentiles_mean': seasonal_percentiles.mean(dim='dayofyear').fillna(0),\n",
    "    'annual_percentile': annual_percentile.fillna(0),\n",
    "    'annual_max': analysis_data.temperature.max(dim='time').fillna(0),\n",
    "    'annual_min': analysis_data.temperature.min(dim='time').fillna(0)\n",
    "}\n",
    "\n",
    "results_ds = xr.Dataset(results)\n",
    "\n",
    "# Add proper CRS metadata\n",
    "results_ds.latitude.attrs.update({\n",
    "    'standard_name': 'latitude',\n",
    "    'long_name': 'latitude',\n",
    "    'units': 'degrees_north',\n",
    "    'axis': 'Y'\n",
    "})\n",
    "\n",
    "results_ds.longitude.attrs.update({\n",
    "    'standard_name': 'longitude',\n",
    "    'long_name': 'longitude', \n",
    "    'units': 'degrees_east',\n",
    "    'axis': 'X'\n",
    "})\n",
    "\n",
    "# Add CRS variable\n",
    "crs = xr.DataArray(\n",
    "    data=np.int32(1),\n",
    "    attrs={\n",
    "        'grid_mapping_name': 'latitude_longitude',\n",
    "        'longitude_of_prime_meridian': 0.0,\n",
    "        'semi_major_axis': 6378137.0,\n",
    "        'inverse_flattening': 298.257223563,\n",
    "        'spatial_ref': 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]]'\n",
    "    }\n",
    ")\n",
    "results_ds['crs'] = crs\n",
    "\n",
    "# Add grid_mapping to data variables\n",
    "for var_name in results_ds.data_vars:\n",
    "    if var_name != 'crs':\n",
    "        results_ds[var_name].attrs['grid_mapping'] = 'crs'\n",
    "\n",
    "# Add global metadata\n",
    "results_ds.attrs.update({\n",
    "    'title': 'Seasonal Extreme Heat Days Analysis',\n",
    "    'analysis_year': CONFIG['analysis_year'],\n",
    "    'reference_period': f'{CONFIG[\"reference_start\"]}-{CONFIG[\"reference_end\"]}',\n",
    "    'created': datetime.now().isoformat(),\n",
    "    'absolute_threshold': CONFIG['absolute_threshold'],\n",
    "    'percentile_threshold': CONFIG['percentile_threshold'],\n",
    "    'methodology': 'seasonal_percentile_day_of_year',\n",
    "    'formula': 'Heat_Day = LST_max > max(absolute_threshold, seasonal_percentile)',\n",
    "    'crs': 'EPSG:4326',\n",
    "    'temperature_filter': f'Applied max temperature filter: {CONFIG[\"max_temp_filter\"]}¬∞C'\n",
    "})\n",
    "\n",
    "# Save NetCDF\n",
    "netcdf_filename = f'{output_dir}/seasonal_heat_analysis_final_{CONFIG[\"analysis_year\"]}.nc'\n",
    "results_ds.to_netcdf(netcdf_filename)\n",
    "\n",
    "file_size_mb = os.path.getsize(netcdf_filename) / 1024**2\n",
    "print(f'   ‚úÖ NetCDF saved: {os.path.basename(netcdf_filename)} ({file_size_mb:.1f} MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Export summary CSV\n",
    "print('   üìä Creating summary CSV...')\n",
    "\n",
    "summary_data = [\n",
    "    {\n",
    "        'method': 'seasonal',\n",
    "        'mean_heat_days': seasonal_mean,\n",
    "        'max_heat_days': seasonal_max,\n",
    "        'pixels_with_heat_days': seasonal_pixels,\n",
    "        'total_pixels': total_pixels,\n",
    "        'percentage_with_heat': seasonal_pixels / total_pixels * 100\n",
    "    },\n",
    "    {\n",
    "        'method': 'annual',\n",
    "        'mean_heat_days': annual_mean,\n",
    "        'max_heat_days': annual_max, \n",
    "        'pixels_with_heat_days': annual_pixels,\n",
    "        'total_pixels': total_pixels,\n",
    "        'percentage_with_heat': annual_pixels / total_pixels * 100\n",
    "    }\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_filename = f'{output_dir}/method_comparison_final_{CONFIG[\"analysis_year\"]}.csv'\n",
    "summary_df.to_csv(summary_filename, index=False)\n",
    "\n",
    "print(f'   ‚úÖ Summary CSV saved: {os.path.basename(summary_filename)}')\n",
    "print('\\nüìä METHOD COMPARISON SUMMARY:')\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create comprehensive documentation\n",
    "print('   üìù Creating methodology documentation...')\n",
    "\n",
    "doc_content = f\"\"\"SEASONAL EXTREME HEAT DAYS ANALYSIS - FINAL REPORT\n",
    "=================================================\n",
    "Generated: {datetime.now().isoformat()}\n",
    "Analysis Year: {CONFIG['analysis_year']}\n",
    "Reference Period: {CONFIG['reference_start']}-{CONFIG['reference_end']}\n",
    "\n",
    "METHODOLOGY:\n",
    "============\n",
    "Formula: Surface_Heat_Day = 1 if LST_daily_max > max(LST_abs, LST_rel)\n",
    "\n",
    "Where:\n",
    "- LST_abs = {CONFIG['absolute_threshold']}¬∞C (absolute threshold)\n",
    "- LST_rel = {CONFIG['percentile_threshold']}th percentile of temperatures for each calendar day\n",
    "- Reference period: {CONFIG['reference_end'] - CONFIG['reference_start'] + 1} years of historical data\n",
    "\n",
    "SCIENTIFIC RATIONALE:\n",
    "====================\n",
    "1. Seasonal Context: Each day compared to its climatological normal\n",
    "2. Climatological Accuracy: Follows meteorological best practices\n",
    "3. Sensitive Detection: Can identify seasonal heat anomalies\n",
    "4. Day-specific Baselines: Accounts for natural seasonal temperature cycles\n",
    "\n",
    "DATA PROCESSING:\n",
    "===============\n",
    "- Data Source: GSHTD (Global Surface Heat Temperature Database)\n",
    "- Spatial Resolution: {CONFIG['scale']}m\n",
    "- Temporal Coverage: {CONFIG['reference_start']}-{CONFIG['analysis_year']}\n",
    "- Quality Control: Temperature values > {CONFIG['max_temp_filter']}¬∞C removed\n",
    "- Extraction Method: Monthly chunking for large dataset handling\n",
    "\n",
    "RESULTS SUMMARY:\n",
    "===============\n",
    "Study Area: Salvador, Brazil region ({area_km2:.1f} km¬≤)\n",
    "Total Pixels Analyzed: {total_pixels:,}\n",
    "Total Observations Processed: {extraction_stats['total_obs']:,}\n",
    "\n",
    "Seasonal Method:\n",
    "- Mean heat days: {seasonal_mean:.1f}\n",
    "- Max heat days: {seasonal_max:.0f}\n",
    "- Pixels with heat days: {seasonal_pixels:,} ({seasonal_pixels/total_pixels*100:.1f}%)\n",
    "\n",
    "Annual Method (comparison):\n",
    "- Mean heat days: {annual_mean:.1f}\n",
    "- Max heat days: {annual_max:.0f}\n",
    "- Pixels with heat days: {annual_pixels:,} ({annual_pixels/total_pixels*100:.1f}%)\n",
    "\n",
    "METHODOLOGICAL COMPARISON:\n",
    "=========================\n",
    "- Mean difference: {seasonal_mean - annual_mean:.1f} days\n",
    "- Correlation: {correlation:.3f}\n",
    "- Affected pixels: {abs(seasonal_pixels - annual_pixels):,} difference\n",
    "\n",
    "COORDINATE REFERENCE SYSTEM:\n",
    "===========================\n",
    "- CRS: EPSG:4326 (WGS84 Geographic)\n",
    "- Units: Decimal degrees\n",
    "- Datum: World Geodetic System 1984\n",
    "- NetCDF includes proper CF-compliant CRS metadata\n",
    "\n",
    "FILES GENERATED:\n",
    "===============\n",
    "1. {os.path.basename(netcdf_filename)} - NetCDF spatial dataset with proper CRS\n",
    "2. {os.path.basename(summary_filename)} - Method comparison summary\n",
    "3. seasonal_heat_methodology_final_{CONFIG['analysis_year']}.txt - This documentation\n",
    "\n",
    "CONCLUSION:\n",
    "==========\n",
    "The seasonal methodology provides more climatologically appropriate \n",
    "extreme heat detection by accounting for natural seasonal temperature cycles.\n",
    "This approach offers improved sensitivity to seasonal heat anomalies compared \n",
    "to traditional annual percentile methods.\n",
    "\n",
    "Analysis completed successfully with {extraction_stats['successful']} successful \n",
    "monthly extractions out of {extraction_stats['successful'] + extraction_stats['failed']} attempted.\n",
    "\"\"\"\n",
    "\n",
    "doc_filename = f'{output_dir}/seasonal_heat_methodology_final_{CONFIG[\"analysis_year\"]}.txt'\n",
    "with open(doc_filename, 'w') as f:\n",
    "    f.write(doc_content)\n",
    "\n",
    "print(f'   ‚úÖ Documentation saved: {os.path.basename(doc_filename)}')\n",
    "\n",
    "print(f'\\nüéØ ANALYSIS COMPLETE!')\n",
    "print(f'üìÅ All outputs saved to: {output_dir}')\n",
    "print(f'\\nüìä Files created:')\n",
    "print(f'   ‚Ä¢ NetCDF spatial dataset with proper CRS')\n",
    "print(f'   ‚Ä¢ Method comparison CSV')\n",
    "print(f'   ‚Ä¢ Comprehensive methodology documentation')\n",
    "print(f'\\n‚úÖ Ready for presentation!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully implements a **climatologically appropriate** extreme heat day calculation using seasonal percentiles. The key advantages of this approach include:\n",
    "\n",
    "### üî¨ **Methodological Improvements:**\n",
    "- **Seasonal context**: Each day compared to its climatological normal\n",
    "- **Day-specific baselines**: Accounts for natural seasonal temperature cycles\n",
    "- **Enhanced sensitivity**: Better detection of seasonal heat anomalies\n",
    "\n",
    "### üìä **Technical Implementation:**\n",
    "- **Clean xarray operations**: Leverages xarray's built-in climate analysis capabilities\n",
    "- **Efficient data handling**: Monthly chunking for large dataset processing\n",
    "- **Proper CRS handling**: CF-compliant NetCDF export for GIS compatibility\n",
    "\n",
    "### üéØ **Scientific Value:**\n",
    "- **Meteorologically sound**: Follows established climatological practices\n",
    "- **Comparative analysis**: Direct comparison with traditional annual methods\n",
    "- **Comprehensive output**: Multiple export formats for further analysis\n",
    "\n",
    "The seasonal methodology represents a significant improvement over simple annual percentiles for extreme heat analysis in climate studies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
