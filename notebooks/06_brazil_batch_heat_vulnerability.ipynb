{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazil-Wide Heat Vulnerability Analysis\n",
    "\n",
    "Batch processing of all Brazilian cities using Google Earth Engine map-reduce to calculate heat vulnerability ratios and demographic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GEE initialized for Brazil-wide processing\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Initialize GEE\n",
    "ee.Initialize(project='tl-cities')\n",
    "print('✅ GEE initialized for Brazil-wide processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cities globally: 11422\n",
      "WorldPop 2020 bands: 37\n",
      "✅ Datasets loaded\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "allCities = ee.FeatureCollection('projects/tl-cities/assets/GHS_UCDB_THEME_HAZARD_RISK_GLOBE_R2024A')\n",
    "worldpopCollection = ee.ImageCollection('WorldPop/GP/100m/pop_age_sex')\n",
    "\n",
    "# Get WorldPop data for 2020 (confirmed available year)\n",
    "worldpop_2020 = worldpopCollection.filter(ee.Filter.eq('year', 2020)).mosaic()\n",
    "\n",
    "print(f\"Total cities globally: {allCities.size().getInfo()}\")\n",
    "print(f\"WorldPop 2020 bands: {len(worldpop_2020.bandNames().getInfo())}\")\n",
    "print('✅ Datasets loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brazilian Cities Filtering and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🇧🇷 Filtering Brazilian cities...\n",
      "✅ Found 200 Brazilian cities\n",
      "   Minimum population: 50,000\n",
      "   Maximum cities: 200\n",
      "\n",
      "📊 Top 10 cities by population:\n",
      "   1. São Paulo            - Pop: 19,485,158, Area: 2111.0 km²\n",
      "   2. Rio de Janeiro       - Pop:  9,853,693, Area: 1285.0 km²\n",
      "   3. Belo Horizonte       - Pop:  4,376,747, Area:  609.0 km²\n",
      "   4. Recife               - Pop:  3,847,558, Area:  491.0 km²\n",
      "   5. Fortaleza            - Pop:  3,324,149, Area:  413.0 km²\n",
      "   6. Salvador             - Pop:  3,305,396, Area:  323.0 km²\n",
      "   7. Porto Alegre         - Pop:  2,763,350, Area:  599.0 km²\n",
      "   8. Curitiba             - Pop:  2,512,877, Area:  566.0 km²\n",
      "   9. Goiânia              - Pop:  2,457,483, Area:  531.0 km²\n",
      "  10. Manaus               - Pop:  2,368,805, Area:  272.0 km²\n",
      "\n",
      "🎯 Ready to process 200 Brazilian cities\n"
     ]
    }
   ],
   "source": [
    "def get_filtered_brazilian_cities(min_population=50000, max_cities=200):\n",
    "    \"\"\"Get filtered Brazilian cities for batch processing\"\"\"\n",
    "    print(f'🇧🇷 Filtering Brazilian cities...')\n",
    "    \n",
    "    # Filter Brazilian cities with population threshold\n",
    "    brazilian_cities = (allCities\n",
    "        .filter(ee.Filter.eq('GC_CNT_GAD', 'Brazil'))\n",
    "        .filter(ee.Filter.gt('GC_POP_TOT', min_population))\n",
    "        .sort('GC_POP_TOT', False)  # Largest cities first\n",
    "        .limit(max_cities)\n",
    "    )\n",
    "    \n",
    "    # Get basic statistics\n",
    "    total_count = brazilian_cities.size().getInfo()\n",
    "    \n",
    "    print(f'✅ Found {total_count} Brazilian cities')\n",
    "    print(f'   Minimum population: {min_population:,}')\n",
    "    print(f'   Maximum cities: {max_cities}')\n",
    "    \n",
    "    # Show sample of top cities\n",
    "    sample = brazilian_cities.limit(10).getInfo()\n",
    "    print(f'\\n📊 Top 10 cities by population:')\n",
    "    for i, city in enumerate(sample['features']):\n",
    "        props = city['properties']\n",
    "        name = props.get('GC_UCN_MAI', 'Unknown')\n",
    "        pop = props.get('GC_POP_TOT', 0)\n",
    "        area = props.get('GC_UCA_KM2', 0)\n",
    "        print(f'  {i+1:2d}. {name:20s} - Pop: {pop:>10,.0f}, Area: {area:>6.1f} km²')\n",
    "    \n",
    "    return brazilian_cities\n",
    "\n",
    "# Test the filtering\n",
    "brazilian_cities = get_filtered_brazilian_cities()\n",
    "print(f'\\n🎯 Ready to process {brazilian_cities.size().getInfo()} Brazilian cities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-Reduce Functions for Heat Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Map-reduce function created\n"
     ]
    }
   ],
   "source": [
    "def create_heat_vulnerability_map_function():\n",
    "    \"\"\"Create the map function for processing individual cities\"\"\"\n",
    "    \n",
    "    def map_city_heat_vulnerability(city_feature):\n",
    "        \"\"\"Map function: Calculate heat vulnerability statistics for one city\"\"\"\n",
    "        \n",
    "        # Get city properties and geometry\n",
    "        city_geometry = city_feature.geometry()\n",
    "        \n",
    "        # Define age cohorts using WorldPop bands\n",
    "        age_cohort_bands = {\n",
    "            'age_0_4': ['M_0', 'M_1', 'F_0', 'F_1'],\n",
    "            'age_5_9': ['M_5', 'F_5'],\n",
    "            'age_10_14': ['M_10', 'F_10'],\n",
    "            'age_15_19': ['M_15', 'F_15'],\n",
    "            'age_20_24': ['M_20', 'F_20'],\n",
    "            'age_25_29': ['M_25', 'F_25'],\n",
    "            'age_30_34': ['M_30', 'F_30'],\n",
    "            'age_35_39': ['M_35', 'F_35'],\n",
    "            'age_40_44': ['M_40', 'F_40'],\n",
    "            'age_45_49': ['M_45', 'F_45'],\n",
    "            'age_50_54': ['M_50', 'F_50'],\n",
    "            'age_55_59': ['M_55', 'F_55'],\n",
    "            'age_60_64': ['M_60', 'F_60'],\n",
    "            'age_65_69': ['M_65', 'F_65'],\n",
    "            'age_70_74': ['M_70', 'F_70'],\n",
    "            'age_75_79': ['M_75', 'F_75'],\n",
    "            'age_80_plus': ['M_80', 'F_80'],\n",
    "            'heat_vuln_ratio': ['M_0', 'M_1', 'F_0', 'F_1', 'M_65', 'F_65', 'M_70', 'F_70', 'M_75', 'F_75', 'M_80', 'F_80']\n",
    "        }\n",
    "        \n",
    "        # Create total population image (all bands except duplicates in heat_vuln_ratio)\n",
    "        all_unique_bands = []\n",
    "        for cohort_name, bands in age_cohort_bands.items():\n",
    "            if cohort_name != 'heat_vuln_ratio':\n",
    "                all_unique_bands.extend(bands)\n",
    "        \n",
    "        all_unique_bands = list(dict.fromkeys(all_unique_bands))  # Remove duplicates\n",
    "        total_pop_image = worldpop_2020.select(all_unique_bands).reduce(ee.Reducer.sum())\n",
    "        \n",
    "        # Calculate statistics for each cohort\n",
    "        cohort_stats = {}\n",
    "        \n",
    "        for cohort_name, bands in age_cohort_bands.items():\n",
    "            # Create cohort population image\n",
    "            cohort_image = worldpop_2020.select(bands).reduce(ee.Reducer.sum())\n",
    "            \n",
    "            # Calculate percentage: (cohort / total) * 100\n",
    "            # Add small constant to avoid division by zero\n",
    "            percentage_image = cohort_image.divide(total_pop_image.add(0.001)).multiply(100)\n",
    "            \n",
    "            # Calculate comprehensive statistics within city boundary\n",
    "            stats = percentage_image.reduceRegion(\n",
    "                reducer=(\n",
    "                    ee.Reducer.mean().combine(ee.Reducer.median(), sharedInputs=True)\n",
    "                    .combine(ee.Reducer.stdDev(), sharedInputs=True)\n",
    "                    .combine(ee.Reducer.minMax(), sharedInputs=True)\n",
    "                    .combine(ee.Reducer.percentile([25, 75]), sharedInputs=True)\n",
    "                    .combine(ee.Reducer.count(), sharedInputs=True)\n",
    "                ),\n",
    "                geometry=city_geometry,\n",
    "                scale=90,  # WorldPop native resolution\n",
    "                maxPixels=1e8,\n",
    "                bestEffort=True\n",
    "            )\n",
    "            \n",
    "            # Extract statistics with proper naming\n",
    "            cohort_stats[f'{cohort_name}_mean'] = stats.get('sum_mean')\n",
    "            cohort_stats[f'{cohort_name}_median'] = stats.get('sum_median')\n",
    "            cohort_stats[f'{cohort_name}_std'] = stats.get('sum_stdDev')\n",
    "            cohort_stats[f'{cohort_name}_min'] = stats.get('sum_min')\n",
    "            cohort_stats[f'{cohort_name}_max'] = stats.get('sum_max')\n",
    "            cohort_stats[f'{cohort_name}_q25'] = stats.get('sum_p25')\n",
    "            cohort_stats[f'{cohort_name}_q75'] = stats.get('sum_p75')\n",
    "            cohort_stats[f'{cohort_name}_count'] = stats.get('sum_count')\n",
    "        \n",
    "        # Also calculate total population for the city\n",
    "        total_pop_stats = total_pop_image.reduceRegion(\n",
    "            reducer=ee.Reducer.sum(),\n",
    "            geometry=city_geometry,\n",
    "            scale=90,\n",
    "            maxPixels=1e8,\n",
    "            bestEffort=True\n",
    "        )\n",
    "        \n",
    "        cohort_stats['total_worldpop_population'] = total_pop_stats.get('sum')\n",
    "        \n",
    "        # Return city feature with computed statistics as properties\n",
    "        return city_feature.set(cohort_stats)\n",
    "    \n",
    "    return map_city_heat_vulnerability\n",
    "\n",
    "print('✅ Map-reduce function created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch processing functions ready\n"
     ]
    }
   ],
   "source": [
    "def process_cities_batch(cities_collection, batch_size=10):\n",
    "    \"\"\"Process a batch of cities and return results as pandas DataFrame\"\"\"\n",
    "    \n",
    "    print(f'🔄 Processing batch of {batch_size} cities...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create the map function\n",
    "        map_function = create_heat_vulnerability_map_function()\n",
    "        \n",
    "        # Apply map function to all cities in batch\n",
    "        cities_with_stats = cities_collection.map(map_function)\n",
    "        \n",
    "        # Get the results\n",
    "        print('   📊 Computing statistics...')\n",
    "        results = cities_with_stats.getInfo()\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        batch_data = []\n",
    "        \n",
    "        for city in results['features']:\n",
    "            props = city['properties']\n",
    "            \n",
    "            # Extract city basic info\n",
    "            city_info = {\n",
    "                'city_name': props.get('GC_UCN_MAI', 'Unknown'),\n",
    "                'country': props.get('GC_CNT_GAD', 'Unknown'),\n",
    "                'population_estimate': props.get('GC_POP_TOT', 0),\n",
    "                'area_km2': props.get('GC_UCA_KM2', 0),\n",
    "                'total_worldpop_population': props.get('total_worldpop_population', 0)\n",
    "            }\n",
    "            \n",
    "            # Extract all cohort statistics\n",
    "            for key, value in props.items():\n",
    "                if any(key.startswith(prefix) for prefix in [\n",
    "                    'age_0_4_', 'age_5_9_', 'age_10_14_', 'age_15_19_', 'age_20_24_',\n",
    "                    'age_25_29_', 'age_30_34_', 'age_35_39_', 'age_40_44_', 'age_45_49_',\n",
    "                    'age_50_54_', 'age_55_59_', 'age_60_64_', 'age_65_69_', 'age_70_74_',\n",
    "                    'age_75_79_', 'age_80_plus_', 'heat_vuln_ratio_'\n",
    "                ]):\n",
    "                    city_info[key] = value\n",
    "            \n",
    "            batch_data.append(city_info)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(batch_data)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f'   ✅ Batch completed in {processing_time:.1f} seconds')\n",
    "        print(f'   📊 Processed {len(df)} cities successfully')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'   ❌ Batch processing failed: {e}')\n",
    "        return None\n",
    "\n",
    "def process_all_brazilian_cities_in_batches(batch_size=10, max_batches=None):\n",
    "    \"\"\"Process all Brazilian cities in manageable batches\"\"\"\n",
    "    \n",
    "    print(f'🚀 Starting Brazil-wide heat vulnerability analysis')\n",
    "    print(f'   Batch size: {batch_size} cities')\n",
    "    print(f'   Max batches: {max_batches or \"All\"}')\n",
    "    print('='*60)\n",
    "    \n",
    "    # Get filtered cities\n",
    "    cities = get_filtered_brazilian_cities()\n",
    "    total_cities = cities.size().getInfo()\n",
    "    total_batches = (total_cities + batch_size - 1) // batch_size\n",
    "    \n",
    "    if max_batches:\n",
    "        total_batches = min(total_batches, max_batches)\n",
    "    \n",
    "    print(f'\\n📊 Processing plan:')\n",
    "    print(f'   Total cities: {total_cities}')\n",
    "    print(f'   Total batches: {total_batches}')\n",
    "    print(f'   Cities per batch: {batch_size}')\n",
    "    \n",
    "    all_results = []\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for batch_idx in range(total_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        \n",
    "        print(f'\\n📦 Batch {batch_idx + 1}/{total_batches} (Cities {start_idx + 1}-{min(start_idx + batch_size, total_cities)})')\n",
    "        \n",
    "        try:\n",
    "            # Get batch of cities\n",
    "            batch_cities_list = cities.toList(batch_size, start_idx)\n",
    "            batch_cities = ee.FeatureCollection(batch_cities_list)\n",
    "            \n",
    "            # Process batch\n",
    "            batch_df = process_cities_batch(batch_cities, batch_size)\n",
    "            \n",
    "            if batch_df is not None and len(batch_df) > 0:\n",
    "                all_results.append(batch_df)\n",
    "                print(f'   ✅ Batch {batch_idx + 1} successful - {len(batch_df)} cities')\n",
    "            else:\n",
    "                print(f'   ❌ Batch {batch_idx + 1} failed or returned no data')\n",
    "            \n",
    "            # Add delay to respect GEE rate limits\n",
    "            if batch_idx < total_batches - 1:  # Don't delay after last batch\n",
    "                print('   ⏳ Waiting 3 seconds before next batch...')\n",
    "                time.sleep(3)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'   ❌ Batch {batch_idx + 1} error: {e}')\n",
    "            continue\n",
    "    \n",
    "    # Combine all successful batches\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Calculate processing summary\n",
    "        end_time = datetime.now()\n",
    "        total_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f'\\n🎉 Brazil-wide processing completed!')\n",
    "        print(f'   Total cities processed: {len(final_df)}')\n",
    "        print(f'   Successful batches: {len(all_results)}/{total_batches}')\n",
    "        print(f'   Total processing time: {total_time:.1f} seconds')\n",
    "        print(f'   Average time per city: {total_time/len(final_df):.1f} seconds')\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print('\\n❌ No successful batches - processing failed')\n",
    "        return None\n",
    "\n",
    "print('✅ Batch processing functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Test: Process Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 PROTOTYPE TEST: Processing 5 largest Brazilian cities\n",
      "============================================================\n",
      "🚀 Starting Brazil-wide heat vulnerability analysis\n",
      "   Batch size: 5 cities\n",
      "   Max batches: 1\n",
      "============================================================\n",
      "🇧🇷 Filtering Brazilian cities...\n",
      "✅ Found 200 Brazilian cities\n",
      "   Minimum population: 50,000\n",
      "   Maximum cities: 200\n",
      "\n",
      "📊 Top 10 cities by population:\n",
      "   1. São Paulo            - Pop: 19,485,158, Area: 2111.0 km²\n",
      "   2. Rio de Janeiro       - Pop:  9,853,693, Area: 1285.0 km²\n",
      "   3. Belo Horizonte       - Pop:  4,376,747, Area:  609.0 km²\n",
      "   4. Recife               - Pop:  3,847,558, Area:  491.0 km²\n",
      "   5. Fortaleza            - Pop:  3,324,149, Area:  413.0 km²\n",
      "   6. Salvador             - Pop:  3,305,396, Area:  323.0 km²\n",
      "   7. Porto Alegre         - Pop:  2,763,350, Area:  599.0 km²\n",
      "   8. Curitiba             - Pop:  2,512,877, Area:  566.0 km²\n",
      "   9. Goiânia              - Pop:  2,457,483, Area:  531.0 km²\n",
      "  10. Manaus               - Pop:  2,368,805, Area:  272.0 km²\n",
      "\n",
      "📊 Processing plan:\n",
      "   Total cities: 200\n",
      "   Total batches: 1\n",
      "   Cities per batch: 5\n",
      "\n",
      "📦 Batch 1/1 (Cities 1-5)\n",
      "🔄 Processing batch of 5 cities...\n",
      "   📊 Computing statistics...\n",
      "   ✅ Batch completed in 18.5 seconds\n",
      "   📊 Processed 5 cities successfully\n",
      "   ✅ Batch 1 successful - 5 cities\n",
      "\n",
      "🎉 Brazil-wide processing completed!\n",
      "   Total cities processed: 5\n",
      "   Successful batches: 1/1\n",
      "   Total processing time: 18.5 seconds\n",
      "   Average time per city: 3.7 seconds\n",
      "\n",
      "📊 PROTOTYPE RESULTS:\n",
      "   Cities processed: 5\n",
      "   Columns: 149\n",
      "\n",
      "🏙️ Cities processed:\n",
      "   1. São Paulo            - Pop: 19,485,158, WorldPop: 21,050,827, Heat Vuln:  14.54%\n",
      "   2. Rio de Janeiro       - Pop:  9,853,693, WorldPop: 10,121,552, Heat Vuln:  14.79%\n",
      "   3. Belo Horizonte       - Pop:  4,376,747, WorldPop:  4,407,157, Heat Vuln:  14.61%\n",
      "   4. Recife               - Pop:  3,847,558, WorldPop:  3,721,243, Heat Vuln:  15.06%\n",
      "   5. Fortaleza            - Pop:  3,324,149, WorldPop:  3,592,734, Heat Vuln:  14.53%\n",
      "\n",
      "🌡️ Heat Vulnerability Statistics:\n",
      "   heat_vuln_ratio_count    : min=49168.00%, max=282671.00%, mean=127658.20%\n",
      "   heat_vuln_ratio_max      : min= 15.33%, max= 21.21%, mean= 17.78%\n",
      "   heat_vuln_ratio_mean     : min= 14.53%, max= 15.06%, mean= 14.71%\n",
      "   heat_vuln_ratio_median   : min= 14.48%, max= 15.96%, mean= 15.26%\n",
      "   heat_vuln_ratio_min      : min=  0.00%, max=  0.00%, mean=  0.00%\n",
      "   heat_vuln_ratio_q25      : min= 13.59%, max= 14.98%, mean= 14.32%\n",
      "   heat_vuln_ratio_q75      : min= 14.98%, max= 18.51%, mean= 16.51%\n",
      "   heat_vuln_ratio_std      : min=  2.32%, max=  5.99%, mean=  3.30%\n",
      "\n",
      "✅ Prototype test successful! Ready for full-scale processing.\n"
     ]
    }
   ],
   "source": [
    "# Test with a small batch first\n",
    "print('🧪 PROTOTYPE TEST: Processing 5 largest Brazilian cities')\n",
    "print('='*60)\n",
    "\n",
    "# Process just 1 batch of 5 cities for testing\n",
    "prototype_results = process_all_brazilian_cities_in_batches(\n",
    "    batch_size=5, \n",
    "    max_batches=1  # Only process 1 batch for testing\n",
    ")\n",
    "\n",
    "if prototype_results is not None:\n",
    "    print(f'\\n📊 PROTOTYPE RESULTS:')\n",
    "    print(f'   Cities processed: {len(prototype_results)}')\n",
    "    print(f'   Columns: {len(prototype_results.columns)}')\n",
    "    \n",
    "    # Show basic info\n",
    "    print(f'\\n🏙️ Cities processed:')\n",
    "    for idx, row in prototype_results.iterrows():\n",
    "        city_name = row['city_name']\n",
    "        pop_est = row['population_estimate']\n",
    "        worldpop_total = row['total_worldpop_population']\n",
    "        heat_vuln = row.get('heat_vuln_ratio_mean', 'N/A')\n",
    "        \n",
    "        print(f'   {idx+1}. {city_name:20s} - Pop: {pop_est:>10,.0f}, WorldPop: {worldpop_total:>10,.0f}, Heat Vuln: {heat_vuln:>6.2f}%')\n",
    "    \n",
    "    # Show sample of heat vulnerability statistics\n",
    "    heat_vuln_cols = [col for col in prototype_results.columns if 'heat_vuln_ratio_' in col]\n",
    "    if heat_vuln_cols:\n",
    "        print(f'\\n🌡️ Heat Vulnerability Statistics:')\n",
    "        for col in heat_vuln_cols:\n",
    "            values = prototype_results[col].dropna()\n",
    "            if len(values) > 0:\n",
    "                print(f'   {col:25s}: min={values.min():6.2f}%, max={values.max():6.2f}%, mean={values.mean():6.2f}%')\n",
    "    \n",
    "    print(f'\\n✅ Prototype test successful! Ready for full-scale processing.')\n",
    "else:\n",
    "    print(f'\\n❌ Prototype test failed - check configuration and retry.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full-Scale Processing Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e7987333a946148cf60c1ccac82215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🇧🇷 Brazil-Wide Heat Vulnerability Processing</h3>'), HTML(value='\\n    <p><stro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🇧🇷 Brazil-wide processing interface ready!\n",
      "Configure batch settings above and click \"Process Brazilian Cities\" to start.\n"
     ]
    }
   ],
   "source": [
    "# Create interface for full-scale processing\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=20,\n",
    "    step=5,\n",
    "    description='Batch Size:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "max_batches_slider = widgets.IntSlider(\n",
    "    value=20,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Max Batches:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='🇧🇷 Process Brazilian Cities',\n",
    "    button_style='primary',\n",
    "    layout={'width': '250px'}\n",
    ")\n",
    "\n",
    "export_button = widgets.Button(\n",
    "    description='💾 Export Results to CSV',\n",
    "    button_style='success',\n",
    "    layout={'width': '200px'},\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "processing_output = widgets.Output()\n",
    "\n",
    "# Global variable to store results\n",
    "brazil_results = None\n",
    "\n",
    "def on_process_click(button):\n",
    "    global brazil_results\n",
    "    \n",
    "    with processing_output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        batch_size = batch_size_slider.value\n",
    "        max_batches = max_batches_slider.value\n",
    "        \n",
    "        print(f'🚀 Starting full-scale Brazil processing')\n",
    "        print(f'   Configuration: {batch_size} cities per batch, max {max_batches} batches')\n",
    "        print(f'   Estimated cities: {batch_size * max_batches}')\n",
    "        \n",
    "        brazil_results = process_all_brazilian_cities_in_batches(\n",
    "            batch_size=batch_size,\n",
    "            max_batches=max_batches\n",
    "        )\n",
    "        \n",
    "        if brazil_results is not None:\n",
    "            print(f'\\n🎉 SUCCESS! Processed {len(brazil_results)} Brazilian cities')\n",
    "            \n",
    "            # Show top cities by heat vulnerability\n",
    "            if 'heat_vuln_ratio_mean' in brazil_results.columns:\n",
    "                top_vulnerable = brazil_results.nlargest(10, 'heat_vuln_ratio_mean')\n",
    "                print(f'\\n🌡️ TOP 10 MOST HEAT VULNERABLE CITIES:')\n",
    "                for idx, row in top_vulnerable.iterrows():\n",
    "                    city = row['city_name']\n",
    "                    vuln = row['heat_vuln_ratio_mean']\n",
    "                    pop = row['population_estimate']\n",
    "                    print(f'   {city:20s}: {vuln:5.2f}% heat vulnerable (Pop: {pop:,.0f})')\n",
    "            \n",
    "            # Enable export button\n",
    "            export_button.disabled = False\n",
    "            \n",
    "        else:\n",
    "            print(f'\\n❌ Processing failed - check logs above for details')\n",
    "\n",
    "def on_export_click(button):\n",
    "    global brazil_results\n",
    "    \n",
    "    if brazil_results is not None:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f'/Users/martynclark/heatInsights-notebooks/data/brazil_heat_vulnerability_{timestamp}.csv'\n",
    "        \n",
    "        brazil_results.to_csv(filename, index=False)\n",
    "        print(f'✅ Results exported to: {filename}')\n",
    "        print(f'   Cities: {len(brazil_results)}')\n",
    "        print(f'   Columns: {len(brazil_results.columns)}')\n",
    "    else:\n",
    "        print('❌ No results to export - run processing first')\n",
    "\n",
    "process_button.on_click(on_process_click)\n",
    "export_button.on_click(on_export_click)\n",
    "\n",
    "# Display interface\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML('<h3>🇧🇷 Brazil-Wide Heat Vulnerability Processing</h3>'),\n",
    "    widgets.HTML('''\n",
    "    <p><strong>Batch process all major Brazilian cities to calculate heat vulnerability ratios.</strong></p>\n",
    "    <p><strong>Configuration:</strong></p>\n",
    "    <ul>\n",
    "        <li><strong>Batch Size:</strong> Number of cities processed simultaneously (5-20)</li>\n",
    "        <li><strong>Max Batches:</strong> Maximum number of batches to process (1-50)</li>\n",
    "        <li><strong>Total Cities:</strong> Batch Size × Max Batches</li>\n",
    "    </ul>\n",
    "    <p><strong>Processing Time:</strong> ~30-60 seconds per batch depending on city sizes</p>\n",
    "    '''),\n",
    "    widgets.HBox([batch_size_slider, max_batches_slider]),\n",
    "    widgets.HBox([process_button, export_button]),\n",
    "    processing_output\n",
    "]))\n",
    "\n",
    "print('\\n🇧🇷 Brazil-wide processing interface ready!')\n",
    "print('Configure batch settings above and click \"Process Brazilian Cities\" to start.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
